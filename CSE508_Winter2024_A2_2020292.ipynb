{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBavLi5h6zsA",
        "outputId": "14dc8937-d0f9-4341-99ad-f2776c1f63fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fuQwAwtW60w0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "yACj86s267ss"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/A2_Data.csv')\n",
        "key = (df['Unnamed: 0'].tolist())\n",
        "df = df.drop(columns=['Unnamed: 0'])\n",
        "df[\"id\"] = key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "gbd74h-R7cI9"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "def fetch_successful_link(links):\n",
        "    for link in links:\n",
        "        try:\n",
        "            response = requests.get(link)\n",
        "            if response.status_code == 200:\n",
        "                return link\n",
        "        except Exception as e:\n",
        "            pass  # Ignore any exceptions and continue to the next link\n",
        "    return None\n",
        "\n",
        "# Convert the string representation to a list using ast.literal_eval\n",
        "# df['Image'] = df['Image'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# # Extract the first link from the 'Image' column\n",
        "# df['First_Link'] = df['Image'].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else x)\n",
        "\n",
        "# Convert the string representation to a list using ast.literal_eval\n",
        "df['Image'] = df['Image'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "df['image'] = df['Image'].apply(lambda x: fetch_successful_link(x) if isinstance(x, list) and len(x) > 0 else x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['image'])"
      ],
      "metadata": {
        "id": "RI8sRXl8h_VQ"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "geqYUiG0zLEX",
        "outputId": "bc69c88b-0409-4fad-aa93-58ba21959eeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Image  \\\n",
              "0    [https://images-na.ssl-images-amazon.com/image...   \n",
              "1    [https://images-na.ssl-images-amazon.com/image...   \n",
              "2    [https://images-na.ssl-images-amazon.com/image...   \n",
              "3    [https://images-na.ssl-images-amazon.com/image...   \n",
              "4    [https://images-na.ssl-images-amazon.com/image...   \n",
              "..                                                 ...   \n",
              "995  [https://images-na.ssl-images-amazon.com/image...   \n",
              "996  [https://images-na.ssl-images-amazon.com/image...   \n",
              "997  [https://images-na.ssl-images-amazon.com/image...   \n",
              "998  [https://images-na.ssl-images-amazon.com/image...   \n",
              "999  [https://images-na.ssl-images-amazon.com/image...   \n",
              "\n",
              "                                           Review Text    id  \\\n",
              "0    Loving these vintage springs on my vintage str...  3452   \n",
              "1    Works great as a guitar bench mat. Not rugged ...  1205   \n",
              "2    We use these for everything from our acoustic ...  1708   \n",
              "3    Great price and good quality.  It didn't quite...  2078   \n",
              "4    I bought this bass to split time as my primary...   801   \n",
              "..                                                 ...   ...   \n",
              "995                 Extremely impressed with this kit.  1265   \n",
              "996  This is a great stereo reverb with plenty of c...  1882   \n",
              "997  I really like the simplicity of this bridge. I...  1547   \n",
              "998  Great Product, but there is no warranty in the...  1004   \n",
              "999  This product is good and is used in profession...  1306   \n",
              "\n",
              "                                                 image  \n",
              "0    https://images-na.ssl-images-amazon.com/images...  \n",
              "1    https://images-na.ssl-images-amazon.com/images...  \n",
              "2    https://images-na.ssl-images-amazon.com/images...  \n",
              "3    https://images-na.ssl-images-amazon.com/images...  \n",
              "4    https://images-na.ssl-images-amazon.com/images...  \n",
              "..                                                 ...  \n",
              "995  https://images-na.ssl-images-amazon.com/images...  \n",
              "996  https://images-na.ssl-images-amazon.com/images...  \n",
              "997  https://images-na.ssl-images-amazon.com/images...  \n",
              "998  https://images-na.ssl-images-amazon.com/images...  \n",
              "999  https://images-na.ssl-images-amazon.com/images...  \n",
              "\n",
              "[994 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-09c67499-1f19-477e-8ca5-79f851b1d93c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>Review Text</th>\n",
              "      <th>id</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>Loving these vintage springs on my vintage str...</td>\n",
              "      <td>3452</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>Works great as a guitar bench mat. Not rugged ...</td>\n",
              "      <td>1205</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>We use these for everything from our acoustic ...</td>\n",
              "      <td>1708</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>Great price and good quality.  It didn't quite...</td>\n",
              "      <td>2078</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>I bought this bass to split time as my primary...</td>\n",
              "      <td>801</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>Extremely impressed with this kit.</td>\n",
              "      <td>1265</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>This is a great stereo reverb with plenty of c...</td>\n",
              "      <td>1882</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>I really like the simplicity of this bridge. I...</td>\n",
              "      <td>1547</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>Great Product, but there is no warranty in the...</td>\n",
              "      <td>1004</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
              "      <td>This product is good and is used in profession...</td>\n",
              "      <td>1306</td>\n",
              "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>994 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09c67499-1f19-477e-8ca5-79f851b1d93c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-09c67499-1f19-477e-8ca5-79f851b1d93c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-09c67499-1f19-477e-8ca5-79f851b1d93c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4f46b029-3224-43fb-a651-fdc3084e733d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f46b029-3224-43fb-a651-fdc3084e733d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4f46b029-3224-43fb-a651-fdc3084e733d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 994,\n  \"fields\": [\n    {\n      \"column\": \"Image\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Review Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 990,\n        \"samples\": [\n          \"I got mine at a music store, not here, but wanted to give a review. First, it is a $40 uke... so people need to keep this in mind. I got it to play a campfires,  the beach, by my hot tub, etc. I am not expecting  it to sound like wood. Given that, I am very pleased. It is a great novelty item. People say theirs does not stay in tune, but mine does.  I can hang it on the wall and not touch it for several months. It  is still pretty much in tune, with some minor adjustment. But I would expect that from a wooden or more expensive one as well.  I added some miniature lights off Amazon and it certainly serves its purpose.  Enough light to see if I want to play in the dark, and also can be festive as well.  I have never regretted purchasing this.\",\n          \"Was excited to get this case at a great price. It was very disappointing to receive the case and find that it would not close and latch completely right out of the box. The top and bottom case just don't seem to fit together right. Seems to be bad quality control.\\n\\nUpdate: I received a return shipment label from Amazon and shipped this back promptly and received confirmation from UPS that it was delivered back to Crossrock on 7/19/17. It's been a week (now 7/26/17) and Crossrock still hasn't refunded and doesn't show that they received the package. So far not impressed with this company's service through Amazon.\\n\\nUpdate: Added a star back since they finally issued a refund.\",\n          \"I love these great finish and quality\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1110,\n        \"min\": 4,\n        \"max\": 3888,\n        \"num_unique_values\": 994,\n        \"samples\": [\n          982,\n          1758,\n          1777\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 981,\n        \"samples\": [\n          \"https://images-na.ssl-images-amazon.com/images/I/71bgu8gW2eL._SY88.jpg\",\n          \"https://images-na.ssl-images-amazon.com/images/I/712JNPt7ufL._SY88.jpg\",\n          \"https://images-na.ssl-images-amazon.com/images/I/61g3QZz9J6L._SY88.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 198
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desired_link = df.loc[df['id'] == 2088, 'image']"
      ],
      "metadata": {
        "id": "2cryj2uQeguh"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desired_link"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGQtNIzEgmrA",
        "outputId": "8c1f2824-8dea-4f3f-a500-6b4b6b6f1cf9"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "860    None\n",
              "Name: image, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image feature extraction"
      ],
      "metadata": {
        "id": "M0qTEIdrd_Y-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "nyLjb4Ff9dKu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Assuming your dataset is stored in a CSV file named 'data.csv'\n",
        "# df = pd.read_csv('data.csv')\n",
        "# Initialize dictionaries to store images and reviews\n",
        "def fetch_image(url, id):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Check for HTTP errors\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching image from {id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Dictionary to store images and reviews\n",
        "image_dict = {}\n",
        "review_dict = {}\n",
        "\n",
        "# Iterate over DataFrame rows\n",
        "for index, row in df.iterrows():\n",
        "    image_id = row['id']\n",
        "    # image_url = (row['Image'])[2:-2]\n",
        "    image_url = (row['image'])\n",
        "    review_text = row['Review Text']\n",
        "    # print(image_url)\n",
        "    # Fetch image and store in dictionary\n",
        "    image = fetch_image(image_url, image_id)\n",
        "    image_dict[image_id] = image\n",
        "\n",
        "    # Store review text in dictionary\n",
        "    review_dict[image_id] = review_text\n",
        "\n",
        "# Accessing images and reviews\n",
        "# for image_id, image in image_dict.items():\n",
        "#     review_text = review_dict[image_id]\n",
        "#     print(f\"Image ID: {image_id}\")\n",
        "#     display(image)  # Display image\n",
        "#     print(f\"Review Text: {review_text}\\n\")\n",
        "\n",
        "# Now, images_dict contains images and reviews_dict contains review texts\n",
        "# You can access them using image ID as keys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "eCG1He3T0Jr3"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageEnhance\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "def preprocess_image(image):\n",
        "    # Check if the image is grayscale or RGB\n",
        "    if image:\n",
        "        is_rgb = image.mode == 'RGB'\n",
        "\n",
        "        # Randomly flip the image horizontally\n",
        "        if is_rgb and random.choice([True, False]):\n",
        "            image = image.transpose(method=Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "        # Randomly adjust brightness\n",
        "        brightness_factor = random.uniform(0.5, 1.5)\n",
        "        enhancer = ImageEnhance.Brightness(image)\n",
        "        image = enhancer.enhance(brightness_factor)\n",
        "\n",
        "        # Randomly adjust contrast\n",
        "        contrast_factor = random.uniform(0.5, 1.5)\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        image = enhancer.enhance(contrast_factor)\n",
        "\n",
        "        # Resize the image to a specific size (e.g., 224x224)\n",
        "        target_size = (224, 224)\n",
        "        image = image.resize(target_size)\n",
        "\n",
        "        # Apply geometric transformations (e.g., rotation)\n",
        "        rotation_angle = random.uniform(-15, 15)\n",
        "        image = image.rotate(rotation_angle)\n",
        "\n",
        "        # Additional pre-processing steps can be added based on your requirements\n",
        "\n",
        "        return image\n",
        "\n",
        "processed_images = {}\n",
        "\n",
        "for image_id, original_image in image_dict.items():\n",
        "    processed_image = preprocess_image(original_image)\n",
        "    processed_images[image_id] = processed_image\n",
        "\n",
        "# Now, 'processed_images' contains the pre-processed images with corresponding IDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jPrR-UINADB6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYCsl6Am_cnk",
        "outputId": "63a50a84-e8cd-455e-f07a-2b40e28dd272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Load pre-trained ResNet model\n",
        "resnet_model = models.resnet50(pretrained=True)\n",
        "resnet_model.eval()\n",
        "\n",
        "# Define a function to preprocess the image and extract features\n",
        "def extract_resnet_features(image):\n",
        "    # Preprocess the image\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image_tensor = preprocess(image)\n",
        "    image_tensor = Variable(image_tensor.unsqueeze(0))\n",
        "\n",
        "    # Extract features using the pre-trained ResNet model\n",
        "    with torch.no_grad():\n",
        "        features = resnet_model(image_tensor)\n",
        "\n",
        "    # Remove the spatial dimensions (flatten the features)\n",
        "    features = features.squeeze().numpy()\n",
        "\n",
        "    # Normalize the features\n",
        "    normalized_features = (features - features.mean()) / features.std()\n",
        "\n",
        "    return normalized_features\n",
        "\n",
        "extracted_features = {}\n",
        "\n",
        "for image_id, image_path in processed_images.items():\n",
        "    try:\n",
        "        features = extract_resnet_features(image_path)\n",
        "        extracted_features[image_id] = features\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image with ID {image_id}: {str(e)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text feature Extraction"
      ],
      "metadata": {
        "id": "JxCvmXD0eFEB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "993E6x9M_2wV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "181f65c2-a5f1-4142-d066-1238a8fd05b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Check if the text is not a string (e.g., if it's a float), then convert to string\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    # Lower-casing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Removing punctuation\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "    # Stop Word Removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Stemming (using Porter Stemmer)\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Lemmatization (using WordNet Lemmatizer)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "\n",
        "preprocessed_texts = {}\n",
        "\n",
        "for text_id, original_text in review_dict.items():\n",
        "    preprocessed_text = preprocess_text(original_text)\n",
        "    preprocessed_texts[text_id] = preprocessed_text\n",
        "\n",
        "# Now, 'preprocessed_texts' contains the pre-processed texts with corresponding IDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = list(preprocessed_texts.values())\n",
        "\n",
        "# Calculate TF-IDF scores\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Create a dictionary to store TF-IDF scores for each document\n",
        "tfidf_scores = {}\n",
        "\n",
        "# Iterate over preprocessed_texts and assign TF-IDF scores\n",
        "for i, text_id in enumerate(preprocessed_texts.keys()):\n",
        "    tfidf_scores[text_id] = tfidf_matrix[i].toarray()[0]"
      ],
      "metadata": {
        "id": "U6zwFa7BJC8d"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_and_review_by_id(dataframe, target_id):\n",
        "    # Search for the entry with the specified ID\n",
        "    result = dataframe[dataframe['id'] == target_id]\n",
        "\n",
        "    # Check if any matching entry is found\n",
        "    if not result.empty:\n",
        "        # Extract image URL and review text\n",
        "        image_url = result['Image'].tolist()  # Assuming the 'Image' column contains lists\n",
        "        review_text = result['Review Text'].values[0]\n",
        "\n",
        "        return image_url, review_text\n",
        "    else:\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "y0fNN8CTBgQS"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMAGE BASED RETREIVAL"
      ],
      "metadata": {
        "id": "6grQ1rROR6M0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def calculate_cosine_similarity(vector1, vector2):\n",
        "    # Convert arrays to NumPy arrays\n",
        "    array1 = np.array(vector1).reshape(1, -1)\n",
        "    array2 = np.array(vector2).reshape(1, -1)\n",
        "    # Calculate cosine similarity\n",
        "    similarity = cosine_similarity(array1, array2)\n",
        "    # Extract the similarity score from the resulting matrix\n",
        "    similarity_score = similarity[0, 0]\n",
        "    return similarity_score\n",
        "\n",
        "\n",
        "def find_top_related_images(input_features, extracted_features, top_n=3):\n",
        "    # Calculate cosine similarity with all images in the dictionary\n",
        "    similarity_scores = {}\n",
        "    for image_id, features in extracted_features.items():\n",
        "        similarity_scores[image_id] = calculate_cosine_similarity(input_features, features)\n",
        "    # Sort images by similarity scores in descending order\n",
        "    sorted_images = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the top N related images\n",
        "    top_related_images = sorted_images[:top_n]\n",
        "    return top_related_images\n",
        "\n",
        "\n",
        "def find_top_related_texts(input_text, tfidf_matrix, top_n=3):\n",
        "    # Calculate TF-IDF scores for the input text\n",
        "    input_tfidf_vector = vectorizer.transform([input_text]).toarray()[0]\n",
        "    # input_tfidf_vector = vectorizer.transform([input_text]).toarray()[0]\n",
        "\n",
        "    # Calculate cosine similarity with all reviews\n",
        "    similarity_scores = {}\n",
        "    for i, review_tfidf_vector in enumerate(tfidf_matrix):\n",
        "        input_tfidf_vector = np.array(input_tfidf_vector).reshape(1, -1)\n",
        "        similarity = cosine_similarity(input_tfidf_vector,  review_tfidf_vector)\n",
        "        similarity_score = similarity[0, 0]\n",
        "        # similarity_scores[i] = calculate_cosine_similarity(input_tfidf_vector, review_tfidf_vector)\n",
        "        similarity_scores[i] = similarity_score\n",
        "\n",
        "    # Sort reviews by similarity scores in descending order\n",
        "    sorted_reviews = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the top N related reviews\n",
        "    top_related_reviews = sorted_reviews[:top_n]\n",
        "    return top_related_reviews"
      ],
      "metadata": {
        "id": "gK-11jVWQy2E"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEXT BASED RETRIEVAL"
      ],
      "metadata": {
        "id": "AyqOgjnPJIAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_imput_url = input(\"Enter image url\")\n",
        "review_input = input(\"Enter review text\")\n",
        "print(\"USING IMAGE BASED RETRIEVAL\")\n",
        "input_image = fetch_image(image_imput_url, 1)\n",
        "input_image_features = extract_resnet_features(preprocess_image(input_image))\n",
        "input_text_preprocessed = preprocess_text(review_input)\n",
        "\n",
        "top_related_images = find_top_related_images(input_image_features, extracted_features)\n",
        "# top_related_reviews = find_top_related_texts(input_text_preprocessed, tfidf_matrix)\n",
        "\n",
        "for image_id, similarity_score in top_related_images:\n",
        "    ## use image id to find image and text from the dataset.\n",
        "    print(f\"Image ID: {image_id}, Similarity Score: {similarity_score}\")\n",
        "    image_url, review_text = get_image_and_review_by_id(df, image_id)\n",
        "    print(f\"Image URL: {image_url}\")\n",
        "    print(f\"Review: {review_text}\")\n",
        "    print(f\"Cosine similarity of images: {similarity_score}\")\n",
        "    ## now find cosine similarity of review.\n",
        "    preprocessed_text = preprocess_text(review_text)\n",
        "    temp = vectorizer.transform([preprocessed_text]).toarray()[0].tolist()\n",
        "    # print(temp)\n",
        "    input_text_preprocessed_tfidf = vectorizer.transform([input_text_preprocessed ]).toarray()[0].tolist()\n",
        "    text_similarity = calculate_cosine_similarity(input_text_preprocessed_tfidf,temp)\n",
        "    print(f\"Cosine similarity of text: {text_similarity}\")\n",
        "    print(f\"Composite Similarity: {(text_similarity + similarity_score)/2}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"USING TEXT BASED RETRIEVAL\")\n",
        "## find input features.\n",
        "input_image = fetch_image(image_imput_url, 1)\n",
        "input_image_features = extract_resnet_features(preprocess_image(input_image))\n",
        "input_text_preprocessed = preprocess_text(review_input)\n",
        "top_related_text = find_top_related_texts(input_text_preprocessed, tfidf_matrix)\n",
        "\n",
        "for image_id, similarity_score in top_related_text:\n",
        "    ## use image id to find image and text from the dataset.\n",
        "    print(f\"Image ID: {image_id}, Similarity Score: {similarity_score}\")\n",
        "    image_url, review_text = get_image_and_review_by_id(df, image_id)\n",
        "    print(f\"Image URL: {image_url}\")\n",
        "    print(f\"Review: {review_text}\")\n",
        "    print(f\"Cosine similarity of text: {similarity_score}\")\n",
        "    input_image = fetch_image(image_imput_url, 1)\n",
        "    input_image_features = extract_resnet_features(preprocess_image(input_image))\n",
        "    related_image = fetch_image(image_url, 1)\n",
        "    related_image_features = extract_resnet_features(preprocess_image(related_image))\n",
        "    image_similarity = calculate_cosine_similarity(related_image_features, input_image_features)\n",
        "    print(f\"Cosine similarity of image: {image_similarity}\")\n",
        "    print(f\"Composite similarity: {(image_similarity + similarity_score)/2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "id": "hrmMfmWVH1fX",
        "outputId": "0cd7bbac-641b-4095-c152-63282b54706e"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter image urlhttps://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg\n",
            "Enter review textI have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring if there is a break.\n",
            "USING IMAGE BASED RETRIEVAL\n",
            "Image ID: 643, Similarity Score: 0.8881058096885681\n",
            "Image URL: [['https://images-na.ssl-images-amazon.com/images/I/719-SDMiOoL._SY88.jpg']]\n",
            "Review: These locking tuners look great and keep tune.  Good quality materials and construction.  Excellent upgrade to any guitar.  I had to drill additions holes for installation.  If your neck already comes with pre-drilled holes, then they should drop right in, otherwise you will need to buy a guitar tuner pin drill jig, also available from Amazon.\n",
            "Cosine similarity of images: 0.8881058096885681\n",
            "Cosine similarity of text: 0.132622763046278\n",
            "Composite Similarity: 0.510364286367423\n",
            "\n",
            "Image ID: 654, Similarity Score: 0.8576998710632324\n",
            "Image URL: [['https://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg']]\n",
            "Review: I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring if there is a break.\n",
            "Cosine similarity of images: 0.8576998710632324\n",
            "Cosine similarity of text: 1.0\n",
            "Composite Similarity: 0.9288499355316162\n",
            "\n",
            "Image ID: 170, Similarity Score: 0.8500685095787048\n",
            "Image URL: [['https://images-na.ssl-images-amazon.com/images/I/711kGbkdzEL._SY88.jpg']]\n",
            "Review: Had to drill into my headstock. Needs 2 holes per tree because of the mounting peg. Use a ruler and a 1/16 drillbit and you'll be fine. I recommend installing with the strings on so you can set them properly.\n",
            "Cosine similarity of images: 0.8500685095787048\n",
            "Cosine similarity of text: 0.011150307316126085\n",
            "Composite Similarity: 0.43060940844741546\n",
            "\n",
            "USING TEXT BASED RETRIEVAL\n",
            "Image ID: 754, Similarity Score: 1.0\n",
            "Image URL: None\n",
            "Review: None\n",
            "Cosine similarity of text: 1.0\n",
            "Error fetching image from 1: Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Unexpected type <class 'NoneType'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-208-89e1c1916695>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0minput_image_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_resnet_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mrelated_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mrelated_image_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_resnet_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelated_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mimage_similarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelated_image_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_image_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cosine similarity of image: {image_similarity}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-201-7dfefcfd0769>\u001b[0m in \u001b[0;36mextract_resnet_features\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     ])\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \"\"\"\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    474\u001b[0m             )\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected type {type(img)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Unexpected type <class 'NoneType'>"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}